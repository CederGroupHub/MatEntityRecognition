{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preparation: load libraries, define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/Tanjin_He/Research/Codes/DataMiningBasedMaterialsSynthesis/Codes/flexible_input\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaterialParser version 3.7\n",
      "Pubchem lookup is on! Will search for unknown materials name in PubChem DB.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import materials_entity_recognition as MER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_parameters():\n",
    "    \"\"\"\n",
    "    Load default configuration. \n",
    "    \"\"\"\n",
    "    # Parse parameters\n",
    "    parameters = OrderedDict()\n",
    "    # \"Tagging scheme (iob or iobes)\"\n",
    "    parameters['tag_scheme'] = 'iobes'\n",
    "    # \"Lowercase words\"\n",
    "    parameters['lower'] = False\n",
    "    # \"Replace digits with 0\"\n",
    "    parameters['zeros'] = False\n",
    "    # \"Token embedding dimension\"\n",
    "    parameters['word_dim'] = 100\n",
    "    # \"Token LSTM hidden layer dimension\"\n",
    "    parameters['word_lstm_dim'] = 100\n",
    "    # \"Use a bidirectional LSTM for words\"\n",
    "    parameters['word_bidirect'] = True\n",
    "    # \"Matrix of pretrained embeddings\"\n",
    "    parameters['pre_emb'] = None\n",
    "    # \"Use CRF (0 to disable)\"\n",
    "    parameters['crf'] = True\n",
    "    # \"Droupout on the input (0 = no dropout)\"\n",
    "    parameters['dropout'] = 0.5\n",
    "    # \"Learning method (SGD, Adadelta, Adam..) and learn rate (0.05 as default)\"\n",
    "    parameters['lr_method'] = \"sgd-lr_.005\"\n",
    "    # \"Input ids of tokens\"\n",
    "    parameters['input_vector'] = True\n",
    "    # \"Input embeddings of tokens\"\n",
    "    parameters['input_matrix'] = False\n",
    "    return parameters\n",
    "\n",
    "def validate_parameters(parameters):\n",
    "    \"\"\"\n",
    "    Make sure the parameters are valid\n",
    "    \"\"\"\n",
    "    assert parameters['word_dim'] > 0\n",
    "    assert 0. <= parameters['dropout'] < 1.0\n",
    "    assert parameters['tag_scheme'] in ['iob', 'iobes']\n",
    "    # input either vector (ids of words in a sentence) or \n",
    "    # matrix (embeddings of words in a sentence)\n",
    "    assert not (parameters[\"input_matrix\"] and parameters[\"input_vector\"])\n",
    "    # if input vector, the dimension of embeddings should be claimed\n",
    "    # the source of embeddings should also be set\n",
    "    if parameters[\"input_vector\"]:\n",
    "        assert (parameters['word_dim'] == parameters['pre_emb'].shape[1]) \n",
    "        \n",
    "def prepare_dataset(sentences, word_to_id, tag_to_id, lower=False):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return a list of lists of dictionaries containing:\n",
    "        - word ids\n",
    "        - tag ids\n",
    "    \"\"\"\n",
    "    def f(x): return x.lower() if lower else x\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        str_words = [w[0] for w in s]\n",
    "        words = []\n",
    "        for tmp_index, w in enumerate(str_words):\n",
    "            if f(w) in word_to_id:\n",
    "                tmp_word = f(w)\n",
    "            else:\n",
    "                tmp_word = '<UNK>'\n",
    "            words.append(word_to_id[tmp_word])\n",
    "\n",
    "        tags = [tag_to_id[w[-1]] for w in s]\n",
    "        data.append({\n",
    "            'str_words': str_words,\n",
    "            'words': words,\n",
    "            'tags': tags,\n",
    "        })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data and set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6601 unique words (137296 in total)\n",
      "{'O': 84890, 'S-Mat': 2688, 'S-Tar': 824, 'S-Pre': 2221}\n",
      "Found 4 unique named entity tags\n",
      "Loading pretrained embeddings from dataset/embedding/embedding_MAT_combine_sg_win5_size100_iter50_noLemma_4.text...\n",
      "Loaded 16926 pretrained embeddings.\n",
      "5928 / 6601 (89.8046%) words have been initialized with pretrained embeddings.\n",
      "5563 found directly, 63 after lowercasing, 59 after lowercasing + zero. 243 after lemma.\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------The main part need to be edited-----------------------------------------------------\n",
    "# data loading\n",
    "# basically, you can use any format you like to load data \n",
    "# here is an example for CoNLL format\n",
    "# The training/validation/test data file consists of many lines, \n",
    "# each of which is a token and its attributes separated by blanks, \n",
    "# e.g.: word O Y tag\n",
    "# the first entry is the token text\n",
    "# the last entry is the tag, such as B-Mat (beginning of material), \n",
    "# I-Tar (intermediate parte of target), O (outside)\n",
    "# other entries between the first and the last can be used to feed pre-engineered features, \n",
    "# which are not used in this example\n",
    "# There is also empty lines, which are used to separate tokens in different sentences\n",
    "# tokens between two empty lines form a sentence\n",
    "\n",
    "# path to training data\n",
    "path_train = \"dataset/test_data/Rs_TP_step2_750_train.text\"\n",
    "# path to develop/validation data\n",
    "path_dev = \"dataset/test_data/Rs_TP_step2_750_dev.text\"\n",
    "# path to test data\n",
    "path_test = \"dataset/test_data/Rs_TP_step2_750_test.text\"\n",
    "\n",
    "# embedding loading\n",
    "# There are three ways to load embeddings\n",
    "# 1. use a file, in which each line is a token and its embeddings,\n",
    "#  e.g.: word 0.0 0.0 0.0 ... 0.0\n",
    "# specify emb_path in this manner\n",
    "# 2. use a dict (like word2vec in gensim)\n",
    "# specify emb_dict in this manner\n",
    "# 3. use a matrix (numpy 2d array), the index of matrix should be consist with the vocab provided\n",
    "# specify params['pre_emb'] in this manner\n",
    "\n",
    "# path to embedding file, here we use the first manner to load embeddings\n",
    "emb_path = \"dataset/embedding/embedding_MAT_combine_sg_win5_size100_iter50_noLemma_4.text\"\n",
    "# dict of embedding, should be provided if the second manner to load embeddings is used\n",
    "emb_dict = None\n",
    "\n",
    "# provide the vocab if you want to use all words in embedding source\n",
    "# in this example, vocab is [] and later the words in the whole dataset \n",
    "# (including training/validation/test sets) are assigned to vocab, \n",
    "# because embedding file is provided\n",
    "# <UNK> is automatically inserted as the first token in vocab\n",
    "vocab = []\n",
    "# set true if reloading model for prediction\n",
    "reload_model = False\n",
    "# should be specified if reloading pre-trained model\n",
    "model_path = None\n",
    "\n",
    "# parameters setting \n",
    "# get default parameters first\n",
    "params = get_default_parameters()\n",
    "# \"Token embedding dimension\"\n",
    "params['word_dim'] = 100\n",
    "# there are two ways to feed inputs\n",
    "# 1. the words in sentences are automatically converted to \n",
    "# a 2d list such as [[w0, w1, w2], [w0, w1, w2]],\n",
    "# where each element is the id of word (index in one-hot vector)\n",
    "# then the embedding is automatically loaded\n",
    "# specify params['input_vector'] = True in this manner\n",
    "# 2. input self-designed features directly (including all things such as embedding and additional features)\n",
    "# the input X should to train() and predict() methods should be like \n",
    "# [\n",
    "#     [\n",
    "#         [w0_emb_0, w0_emb_1, w0_emb_2],    # -> one token \n",
    "#         [w0_emb_0, w0_emb_1, w0_emb_2],    # -> one token\n",
    "#     ]                                      # -> one sentence\n",
    "# ]                                          # -> all sentences\n",
    "# specify params['input_matrix'] = True in this manner\n",
    "\n",
    "# \"Input ids of tokens\"\n",
    "# the first manner to feed inputs is used here\n",
    "params['input_vector'] = True\n",
    "# \"Matrix of pretrained embeddings\"\n",
    "# should be specified as a numpy 2d array if the third manner to load embeddings is used\n",
    "params['pre_emb'] = None\n",
    "# \"Input embeddings of tokens\"\n",
    "# should be specified as True if using the second manner to feed inputs \n",
    "params['input_matrix'] = False\n",
    "\n",
    "\n",
    "#---------------not need to be edited if not changing the format of data files-----------------------------\n",
    "# Data parameters\n",
    "lower = params['lower']\n",
    "zeros = params['zeros']\n",
    "tag_scheme = params['tag_scheme']\n",
    "\n",
    "# Load sentences\n",
    "train_sentences = MER.loader.load_sentences(path_train, lower, zeros)\n",
    "dev_sentences = MER.loader.load_sentences(path_dev, lower, zeros)\n",
    "test_sentences = MER.loader.load_sentences(path_test, lower, zeros)\n",
    "\n",
    "# Use selected tagging scheme (IOB / IOBES)\n",
    "MER.loader.update_tag_scheme(train_sentences, tag_scheme)\n",
    "MER.loader.update_tag_scheme(dev_sentences, tag_scheme)\n",
    "MER.loader.update_tag_scheme(test_sentences, tag_scheme)\n",
    "\n",
    "# Create a dictionary / mapping of words\n",
    "# If we use pretrained embeddings, we add them to the dictionary.\n",
    "if vocab:\n",
    "    if '<UNK>' not in vocab:\n",
    "        vocab.insert(0, '<UNK>')\n",
    "    id_to_word = {i: v for i, v in enumerate(vocab)}\n",
    "    word_to_id = {v: k for k, v in list(id_to_word.items())}\n",
    "elif (emb_path or emb_dict):\n",
    "    vocab, word_to_id, id_to_word = MER.loader.word_mapping(\n",
    "        train_sentences + dev_sentences + test_sentences, lower)\n",
    "else:\n",
    "    vocab, word_to_id, id_to_word = MER.loader.word_mapping(train_sentences, lower)\n",
    "\n",
    "# Create a dictionary and a mapping for words / POS tags / tags\n",
    "dico_tags, tag_to_id, id_to_tag = MER.loader.tag_mapping(train_sentences)\n",
    "    \n",
    "# get embedding matrix\n",
    "if (emb_path or emb_dict) and (not params['pre_emb']):\n",
    "    params['pre_emb'] = MER.loader.prepare_embedding_matrix(id_to_word, params['word_dim'], \n",
    "                                                 emb_path=emb_path, emb_dict=emb_dict)\n",
    "    \n",
    "# ensure thee parameters are valid\n",
    "validate_parameters(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6601, 100) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(params['pre_emb'].shape, type(params['pre_emb']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3477 / 718 / 1067 sentences in train / dev / test.\n",
      "Reloading previous model...\n",
      "Model location: models/model_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Tanjin_He/software/anaconda3/lib/python3.6/site-packages/theano/gpuarray/dnn.py:184: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to a version >= v5 and <= v7.\n",
      "  warnings.warn(\"Your cuDNN version is more recent than \"\n"
     ]
    }
   ],
   "source": [
    "# Index data\n",
    "train_data = prepare_dataset(\n",
    "    train_sentences, word_to_id, tag_to_id, lower\n",
    ")\n",
    "dev_data = prepare_dataset(\n",
    "    dev_sentences, word_to_id, tag_to_id, lower\n",
    ")\n",
    "test_data = prepare_dataset(\n",
    "    test_sentences, word_to_id, tag_to_id, lower\n",
    ")\n",
    "\n",
    "print(\"%i / %i / %i sentences in train / dev / test.\" % (\n",
    "    len(train_data), len(dev_data), len(test_data)))\n",
    "\n",
    "# build model\n",
    "if not reload_model:\n",
    "    # Initialize model\n",
    "    model = MER.Model_train()\n",
    "    print(\"Model location: %s\" % model.model_path)\n",
    "    # Save the mappings to disk\n",
    "    print('Putting the mappings in model and saving in disk...')\n",
    "    model.save_mappings(id_to_word, id_to_tag)\n",
    "    # Build the model\n",
    "    model.build(**params)\n",
    "else:\n",
    "    # Reload previous model values\n",
    "    print('Reloading previous model...')\n",
    "    model = MER.Model_train(model_path=model_path)\n",
    "    print(\"Model location: %s\" % model.model_path)\n",
    "    # Build the model\n",
    "    model.build(pre_emb=params['pre_emb'], **model.parameters)\n",
    "    model.reload()\n",
    "    \n",
    "# train model\n",
    "model.fit(input_X=[d['words'] for d in train_data], \n",
    "          input_Y=[d['tags'] for d in train_data],\n",
    "          dev_X=[d['words'] for d in dev_data], \n",
    "          dev_Y=[d['tags'] for d in dev_data],\n",
    "          dev_sentences=dev_sentences,\n",
    "          test_X=[d['words'] for d in test_data], \n",
    "          test_Y=[d['tags'] for d in test_data],\n",
    "          test_sentences=test_sentences,\n",
    "          n_epochs=3,\n",
    "         )\n",
    "\n",
    "# use trained model to predict\n",
    "label_predictions = model.predict_label([d['words'] for d in test_data])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
